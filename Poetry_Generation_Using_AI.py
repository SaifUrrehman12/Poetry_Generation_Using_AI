# -*- coding: utf-8 -*-
import spacy
unlp = spacy.blank('ur')

from google.colab import drive
drive.mount('/content/drive')



"""# Loading Corpus"""

#Loading the corpus
# import required module
from pathlib import Path
  
# assign directory
directory = '/content/drive/MyDrive/NLP Online/Assignment#03/corpus'
urdu = ""
# itrate over files in
# that directory
files = Path(directory).glob('*')
for file in files:
    f=open(file,encoding='UTF-8')
    urdu = urdu+f.read()
    urdu = urdu + '\n'
    print(urdu)

"""# Data Cleaning And Pre-Processing"""

#Tokenizing the corpus
utext = unlp(urdu)
original_word_tokens = []
for token in utext:
    original_word_tokens.append(token.text)
print(original_word_tokens)

#Data Cleaning
clean_list = ['\n', '\n\n', '\n \n\n', '\n\n\n', "'", '“', '\n\n \n\n\n', '؟', '،', '!', ')', '(', '\n \n\n\n', ', ', '‘', '\n\n\n\n', '٪','۔','\n\n\n ']
original_word_tokens = [s for s in original_word_tokens if s not in clean_list]
print(original_word_tokens)

for i in original_word_tokens:
  print(i)

#Data Pre-Processing
#Some words like سوزودرد و آرزو
#are tokenized seperatory like three words but it should be one word
#so i am preprocessing the data to make it one word
ctr = 0
new_word=""
for word in original_word_tokens:
  if (ctr+1<len(original_word_tokens)):
    if original_word_tokens[ctr+1] == 'و':
      new_word=word+" "+original_word_tokens[ctr+1]+" "+original_word_tokens[ctr+2]
      original_word_tokens.pop(ctr)
      original_word_tokens.pop(ctr)
      original_word_tokens.pop(ctr)
      original_word_tokens.insert(ctr,new_word)
      new_word = ""
  ctr+=1

for i in original_word_tokens:
  print(i)

"""# Unigram Model"""

#finding the frequency distribution/probability of each word
frequency_distribution_dictionary = {}
for item in original_word_tokens:
  if (item in frequency_distribution_dictionary):
    frequency_distribution_dictionary[item] += 1
  else:
    frequency_distribution_dictionary[item] = 1
  
for key, value in frequency_distribution_dictionary.items():
        frequency_distribution_dictionary[key]=value/len(original_word_tokens)



for key, value in frequency_distribution_dictionary.items():
        print(key,value)

"""# Bigram Model"""

original_word_tokens_bigram=[]
ctr2 = 0
w1=""
w2=""
for i in original_word_tokens:
  if(ctr2<len(original_word_tokens)-1):
    w1 = i
    w2 = original_word_tokens[ctr2+1]
    original_word_tokens_bigram.append(w1+" "+w2)
    w1=""
    w2=""
    ctr2+=1

original_word_tokens_bigram

frequency_distribution_dictionary_bigram = {}
for item in original_word_tokens_bigram:
  if (item in frequency_distribution_dictionary_bigram):
    frequency_distribution_dictionary_bigram[item] += 1
  else:
    frequency_distribution_dictionary_bigram[item] = 1
  
#for key, value in frequency_distribution_dictionary_bigram.items():
        #frequency_distribution_dictionary_bigram[key]=value/len(original_word_tokens_bigram)

for key, value in frequency_distribution_dictionary_bigram.items():
        print(key,value)

from collections import defaultdict
cfd = defaultdict(lambda: defaultdict(lambda: 0))
for i in range(len(original_word_tokens) - 2):  # loop to the next-to-last word
    cfd[original_word_tokens[i]][original_word_tokens[i+1]] += 1

# pretty print the defaultdict
{k: dict(v) for k, v in dict(cfd).items()}

print(rword)
max(cfd['یہی'])

"""# Generating Poetry using bigram model"""

print("This poetry is generated by NLP")
print("Run this cell many times, and every time, new poetry will come\n\n")

for i in range(0,3):
  for j in range(0,4):
    rval=random.randrange(0, len(original_word_tokens)-1)
    #print(rval)
    rword = original_word_tokens[rval]
    sent = rword
    temp = rword
    for u in range(0, random.randrange(7,10)):
      temp = max(cfd[temp])
      #print(temp)
      sent =sent + " " + max(cfd[temp])
      #print(temp)
      #print("This is temp: ", temp)
      #print("This is sentence: ",sent)
    print(sent)
    print("\n")
  print("\n\n\n")

"""# Trigram Model"""

from collections import defaultdict
cfd = defaultdict(lambda: defaultdict(lambda: 0))
for i in range(len(original_word_tokens_bigram) - 2):  #here using the list containing two pairs of word
    cfd[original_word_tokens_bigram[i]][original_word_tokens_bigram[i+1]] += 1
{k: dict(v) for k, v in dict(cfd).items()}

"""# Generating Poetry using trigram model"""

print("This poetry is generated by NLP")
print("Run this cell many times, and every time, new poetry will come\n\n")

for i in range(0,3):
  for j in range(0,4):
    rval=random.randrange(0, len(original_word_tokens_bigram)-1)
    #print(rval)
    rword = original_word_tokens_bigram[rval]
    sent = rword
    temp = rword
    for u in range(0, random.randrange(5,7)):
      temp = max(cfd[temp])
      #print(temp)
      sent =sent + " " + max(cfd[temp])
      #print(temp)
      #print("This is temp: ", temp)
      #print("This is sentence: ",sent)
    print(sent)
    print("\n")
  print("\n\n\n")